{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import skipgrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.data import load\n",
    "\n",
    "import random\n",
    "import functools\n",
    "import string\n",
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation\n",
    "In this section the xml data will be converted to a panda dataframe. This is done with the help of a Parser. The parser provides us with:\n",
    "- getText(datalocation): the function is used to extract text tag values from every record. \n",
    "- addFeatureFromText(text,feature,skipString,nextLine,collectNext,featureKeyName): adds features derived from the text tag to the features directory \n",
    "- getDataframe(): returns a populated/empty Dataframe object. It depends on the number features that we already extracted\n",
    "- removeEmptyEntries(dataframeObject, column): removes empty rows or the ones which contain \"NaN\"\n",
    "\n",
    "The XMLDataframeParser methods can be found on: https://github.com/LaertNuhu/MedDataVisualisation/blob/master/py/dataManipulation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py.dataManipulation import XMLDataframeParser\n",
    "\n",
    "parser = XMLDataframeParser()\n",
    "text = parser.getText(\"./data/smokingRecords.xml\")\n",
    "parser.addFeatureFromText(text, \"HISTORY OF PRESENT ILLNESS :\", \"\", True, True,\"illness\")\n",
    "df = parser.getDataframe()\n",
    "df_xml = parser.removeEmptyEntries(df,\"illness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalisation\n",
    "\n",
    "In this section we are going to start preparing for text normalisation. \n",
    "Here we will split the text recived from pandas dataframe into tokens. Transform them into lowercase and remove any characters which are non alfabetical. This is a decision that was made for the simple reason that nonalfabetical information is redundant in this case. For features like 'history course' numerical characters could be important for the analysis, but for the 'history of present illness' feature is not.\n",
    "\n",
    "The text, which is on english, will be cleaned from stop words. At the end we also need only the stem of the words. \n",
    "By normalisation we try to have a clean and uniformal structure of text.\n",
    "\n",
    "The above actions are all covered by [normalize and normalizeArray](https://github.com/LaertNuhu/MedDataVisualisation/blob/dev/py/normalize.py) functions.\n",
    "\n",
    "An example of the functions used by these methods can be seen on the following blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Normalizer class from normalize package\n",
    "from py.normalize import Normalizer\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is an 80-year-old male , who had a history of colon cancer in the past , resected approximately ten years prior to admission , history of heavy alcohol use , who presented with a two week history of poor PO intake , weight loss , and was noted to have acute on chronic Hepatitis by chemistries and question of pyelonephritis .\n"
     ]
    }
   ],
   "source": [
    "# we take a snippet from our dataframe\n",
    "text = df_xml[\"illness\"][1][:338]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'patient', 'is', 'an', '80-year-old', 'male', ',', 'who', 'had', 'a', 'history', 'of', 'colon', 'cancer', 'in', 'the', 'past', ',', 'resected', 'approximately', 'ten', 'years', 'prior', 'to', 'admission', ',', 'history', 'of', 'heavy', 'alcohol', 'use', ',', 'who', 'presented', 'with', 'a', 'two', 'week', 'history', 'of', 'poor', 'PO', 'intake', ',', 'weight', 'loss', ',', 'and', 'was', 'noted', 'to', 'have', 'acute', 'on', 'chronic', 'Hepatitis', 'by', 'chemistries', 'and', 'question', 'of', 'pyelonephritis', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer example\n",
    "#text = \"The 3 quick brown foxes are jumping over the fence. Dog barks. Now there are 3 foxes against the dog.\"\n",
    "tokenizer = normalizer.tokenize(text)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'patient', 'is', 'an', '80-year-old', 'male', ',', 'who', 'had', 'a', 'history', 'of', 'colon', 'cancer', 'in', 'the', 'past', ',', 'resected', 'approximately', 'ten', 'years', 'prior', 'to', 'admission', ',', 'history', 'of', 'heavy', 'alcohol', 'use', ',', 'who', 'presented', 'with', 'a', 'two', 'week', 'history', 'of', 'poor', 'po', 'intake', ',', 'weight', 'loss', ',', 'and', 'was', 'noted', 'to', 'have', 'acute', 'on', 'chronic', 'hepatitis', 'by', 'chemistries', 'and', 'question', 'of', 'pyelonephritis', '.']\n"
     ]
    }
   ],
   "source": [
    "# lowercase transformation\n",
    "lowercaseTokens = normalizer.toLowerCase(tokenizer)\n",
    "print(lowercaseTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'patient', 'is', 'an', '80yearold', 'male', '', 'who', 'had', 'a', 'history', 'of', 'colon', 'cancer', 'in', 'the', 'past', '', 'resected', 'approximately', 'ten', 'years', 'prior', 'to', 'admission', '', 'history', 'of', 'heavy', 'alcohol', 'use', '', 'who', 'presented', 'with', 'a', 'two', 'week', 'history', 'of', 'poor', 'PO', 'intake', '', 'weight', 'loss', '', 'and', 'was', 'noted', 'to', 'have', 'acute', 'on', 'chronic', 'Hepatitis', 'by', 'chemistries', 'and', 'question', 'of', 'pyelonephritis', '']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from token array\n",
    "noPunctuations = normalizer.removePunctuation(tokenizer)\n",
    "print(noPunctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'patient', 'is', 'an', 'male', 'who', 'had', 'a', 'history', 'of', 'colon', 'cancer', 'in', 'the', 'past', 'resected', 'approximately', 'ten', 'years', 'prior', 'to', 'admission', 'history', 'of', 'heavy', 'alcohol', 'use', 'who', 'presented', 'with', 'a', 'two', 'week', 'history', 'of', 'poor', 'PO', 'intake', 'weight', 'loss', 'and', 'was', 'noted', 'to', 'have', 'acute', 'on', 'chronic', 'Hepatitis', 'by', 'chemistries', 'and', 'question', 'of', 'pyelonephritis']\n"
     ]
    }
   ],
   "source": [
    "# remove non alphabetic characters\n",
    "noAlpha = normalizer.removeNonAlpha(tokenizer)\n",
    "print(noAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'patient', '80-year-old', 'male', ',', 'history', 'colon', 'cancer', 'past', ',', 'resected', 'approximately', 'ten', 'years', 'prior', 'admission', ',', 'history', 'heavy', 'alcohol', 'use', ',', 'presented', 'two', 'week', 'history', 'poor', 'PO', 'intake', ',', 'weight', 'loss', ',', 'noted', 'acute', 'chronic', 'Hepatitis', 'chemistries', 'question', 'pyelonephritis', '.']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "noStopWords = normalizer.removeStopWords(tokenizer)\n",
    "print(noStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'patient', 'is', 'an', '80-year-old', 'male', ',', 'who', 'had', 'a', 'histori', 'of', 'colon', 'cancer', 'in', 'the', 'past', ',', 'resect', 'approxim', 'ten', 'year', 'prior', 'to', 'admiss', ',', 'histori', 'of', 'heavi', 'alcohol', 'use', ',', 'who', 'present', 'with', 'a', 'two', 'week', 'histori', 'of', 'poor', 'PO', 'intak', ',', 'weight', 'loss', ',', 'and', 'wa', 'note', 'to', 'have', 'acut', 'on', 'chronic', 'hepat', 'by', 'chemistri', 'and', 'question', 'of', 'pyelonephr', '.']\n"
     ]
    }
   ],
   "source": [
    "# get tokens stemm\n",
    "stemmed = normalizer.stemTokens(tokenizer)\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'patient', 'be', 'an', '80-year-old', 'male', ',', 'who', 'have', 'a', 'history', 'of', 'colon', 'cancer', 'in', 'the', 'past', ',', 'resect', 'approximately', 'ten', 'years', 'prior', 'to', 'admission', ',', 'history', 'of', 'heavy', 'alcohol', 'use', ',', 'who', 'present', 'with', 'a', 'two', 'week', 'history', 'of', 'poor', 'PO', 'intake', ',', 'weight', 'loss', ',', 'and', 'be', 'note', 'to', 'have', 'acute', 'on', 'chronic', 'Hepatitis', 'by', 'chemistries', 'and', 'question', 'of', 'pyelonephritis', '.']\n"
     ]
    }
   ],
   "source": [
    "# get tokens lemma\n",
    "lemmatized = normalizer.lemmatizeVerbs(tokenizer)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "In this section we start with the first step of data analysis. Here we do intial data exploration analysis. \n",
    "\n",
    "Steps:\n",
    " 1. Calculate how many tokens has the document\n",
    " 2. which are the most frequent words\n",
    " 3. calulate words frequency using tf-idf\n",
    " 4. compare the results between 2 and 3\n",
    " 5. which are the most frequent word pairs?\n",
    " 6. do the same as 3 but for word pairs\n",
    " 7. use skipgramms\n",
    " 8. compare results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Total words count\n",
    "The countWords function is used to find out how many tokens are on our dataset after cleaning it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWords(data,lemmatized,POS):\n",
    "    count=0\n",
    "    normalizedArray = normalizer.normalizeArray(data,lemmatized,POS)\n",
    "    for tokenArray in normalizedArray:\n",
    "        for token in tokenArray:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 31632\n"
     ]
    }
   ],
   "source": [
    "# count tokens\n",
    "totalTokenCount = countWords(df_xml[\"illness\"],False,False)\n",
    "print('Total tokens: %d' % (totalTokenCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Word occurence process example\n",
    "1. Create a CountVectorizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. fit data and transform it to a document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1]\n",
      " [0 0 1 1 1]\n",
      " [1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# count vectorizer ignores every token with a length smaller than 2\n",
    "bag_of_words = vectorizer.fit_transform([\"There is a cat\",\"There is a dog\",\"Dog barks\"])\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sum matrix verticaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# axis 0 is the vertical axis\n",
    "# returns a matrix\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "print(sum_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Map tokens to the sum_words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('there', 4), ('is', 3), ('cat', 1), ('dog', 2), ('barks', 0)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary_.items() returns a dictionary with the token and its index\n",
    "# tokens are sorted alphabeticaly in the matrix\n",
    "vectorizer.vocabulary_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of \"there\" is 2\n",
      "count of \"is\" is 2\n",
      "count of \"cat\" is 1\n",
      "count of \"dog\" is 2\n",
      "count of \"barks\" is 1\n"
     ]
    }
   ],
   "source": [
    "# map token to ocunt\n",
    "words_frequency = []\n",
    "for token,idx in vectorizer.vocabulary_.items():\n",
    "    words_frequency.append((token,sum_words[0,idx]))\n",
    "    print('count of \"%s\" is %s' % (token,sum_words[0,idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsorted: [('there', 2), ('is', 2), ('cat', 1), ('dog', 2), ('barks', 1)]\n",
      "Sorted after count: [('there', 2), ('is', 2), ('dog', 2), ('cat', 1), ('barks', 1)]\n"
     ]
    }
   ],
   "source": [
    "# sort words frequency list\n",
    "print(\"Unsorted: %s\" % words_frequency)\n",
    "words_frequency_sorted = sorted(words_frequency, key = lambda x: x[1], reverse=True)\n",
    "print(\"Sorted after count: %s\" % words_frequency_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('there', 2), ('is', 2)]\n",
      "[('there', 2), ('is', 2), ('dog', 2)]\n"
     ]
    }
   ],
   "source": [
    "# limit the list output\n",
    "# based on tokens\n",
    "n = 2 # the first two tokens\n",
    "print(words_frequency_sorted[:n])\n",
    "# based on count\n",
    "t = 2 # tokens with count higher or the same as 2\n",
    "print(list(filter(lambda x: x[1]>=t,words_frequency_sorted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Words occurency using [SKlearn Bib](https://scikit-learn.org/stable/)\n",
    "Quick Info:\n",
    "Using SKlearn CountVectorizer to count word frequencies and as tokenizer we use the normalize function (can be found by data normalisation paragraph). \n",
    "The reason for that is because CountVectorizer doesn't support stemming out of the box. We assign the tokenizer parameter with our normalize function which gives stemmed/lemmatized tokens as a result\n",
    "\n",
    "Content: \n",
    "1. getFirstNWords()\n",
    "2. getFirstWordsWithCountGreater()\n",
    "3. buildPlot()\n",
    "4. Most frequent words Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b 1. getFirstNWords function: \n",
    "- takes the vectorizer, the documents form panda dataframe and the number of rows which will be included\n",
    "- is the function which does the fit and transformation of the data. \n",
    "- the sorting is also done here, giving the top most frequent tokens, n-grams\n",
    "\n",
    "Parameter explenation:\n",
    "- vec: takes an Vectorizer Object\n",
    "- data: takes a data stream. In our case we give here as input the pandas dataframe colum wich we want to analyse\n",
    "- n: stands for the number of results that we want to have. For example the first 10 most frequent words would mean a n of 10\n",
    "\n",
    "returns a list of zips. (a,b) were a is the word/words and b is the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency\n",
    "Quick Info:\n",
    "Usind SKlearn CountVectorizer to count word frequencies and as tokenizer we use the normalize function (can be found by data normalisation paragraph). \n",
    "The reason for that is because CountVectorizer doesn't support stemming out of the box and for that we assign the tokenizer parameter with our function which gives stemmed tokens\n",
    "\n",
    "#### getFirstNWords function: \n",
    "- takes the vectorizer, the documents form panda dataframe and the number of rows which will be included\n",
    "- is the function which does the fit and transformation of the data. \n",
    "- the sorting is also done here, giving the top most frequent tokens, n-grams\n",
    "\n",
    "Parameter explenation:\n",
    "- vec: takes an Vectorizer Object\n",
    "- data: takes a data stream. In our case we give here as input the pandas dataframe colum wich we want to analyse\n",
    "- n: stands for the number of results that we want to have. For example the first 10 most frequent words would mean a n of 10\n",
    "\n",
    "returns a list of zips. (a,b) were a is the word/words and b is the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstNWords(vec,data,n):\n",
    "    bag_of_words = vec.fit_transform(data)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq_sorted[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b 2. getFirstWordsWithCountGreater function:\n",
    "- takes the vectorizer, the documents form panda dataframe and the number of rows which will be included\n",
    "- is the function which does the fit and transformation of the data.\n",
    "- the sorting is also done here, giving the top most frequent tokens whose count is grater than the threshold t, n-grams\n",
    "\n",
    "Parameter explenation:\n",
    "- vec: takes an Vectorizer Object\n",
    "- data: takes a data stream. In our case we give here as input the pandas dataframe colum wich we want to analyse\n",
    "- t: stands for the threshold. For example we want words that have a count grater than 10, in wich case t would be 10\n",
    "\n",
    "returns a list of zips. (a,b) were a is the word/words and b is the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstWordsWithCountGreater(vec,data,t):\n",
    "    bag_of_words = vec.fit_transform(data)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return filter(lambda x: x[1]>=t,words_freq_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b 3. buildPlot function:\n",
    "the buildPlot function as the name already says it creates a plot.\n",
    "\n",
    "Parameter explenation:\n",
    "- freqWords: is a List containing elements (a,b) where a is one word, or many words depending what we want to count and b is the count of the word/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildPlot(freqWords):\n",
    "    labels, values = zip(*freqWords)\n",
    "    indexes = np.arange(len(labels))\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.xticks(indexes, labels)\n",
    "    plt.bar(indexes, values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('tokens')\n",
    "    plt.ylabel('count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b 4. Most frequent words graph\n",
    "\n",
    "Firstly we create a CountVectoizer Object. Then we get the top 10 most frequent words using getFirstNWords function. Last step is to visualize the result using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Firstly creating a CountVectoizer Object\n",
    "vec = CountVectorizer(tokenizer=lambda text: normalizer.normalize(text, True,False))\n",
    "mostFreqWords = getFirstNWords(vec,df_xml.illness,10)\n",
    "buildPlot(mostFreqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_d1eaa880_44ea_11e9_bcde_de007860c501row0_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 100.0%, transparent 100.0%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row1_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 15.9%, transparent 15.9%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row2_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 15.1%, transparent 15.1%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row3_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 12.2%, transparent 12.2%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row4_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 11.1%, transparent 11.1%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row5_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 9.9%, transparent 9.9%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row6_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 2.6%, transparent 2.6%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row7_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,#4286f4 0.8%, transparent 0.8%);\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row8_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }    #T_d1eaa880_44ea_11e9_bcde_de007860c501row9_col0 {\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "        }</style><table id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >count</th>    </tr>    <tr>        <th class=\"index_name level0\" >token</th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row0\" class=\"row_heading level0 row0\" >patient</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row0_col0\" class=\"data row0 col0\" >949</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row1\" class=\"row_heading level0 row1\" >pain</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row1_col0\" class=\"data row1 col0\" >327</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row2\" class=\"row_heading level0 row2\" >year</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row2_col0\" class=\"data row2 col0\" >321</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row3\" class=\"row_heading level0 row3\" >left</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row3_col0\" class=\"data row3 col0\" >299</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row4\" class=\"row_heading level0 row4\" >history</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row4_col0\" class=\"data row4 col0\" >291</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row5\" class=\"row_heading level0 row5\" >right</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row5_col0\" class=\"data row5 col0\" >282</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row6\" class=\"row_heading level0 row6\" >hospital</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row6_col0\" class=\"data row6 col0\" >228</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row7\" class=\"row_heading level0 row7\" >showed</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row7_col0\" class=\"data row7 col0\" >215</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row8\" class=\"row_heading level0 row8\" >admission</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row8_col0\" class=\"data row8 col0\" >209</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501level0_row9\" class=\"row_heading level0 row9\" >old</th>\n",
       "                        <td id=\"T_d1eaa880_44ea_11e9_bcde_de007860c501row9_col0\" class=\"data row9 col0\" >209</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a22513f28>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostFreqWordsDf = pd.DataFrame(mostFreqWords,columns = [\"token\",\"count\"])\n",
    "mostFreqWordsDf.set_index('token', inplace=True)\n",
    "mostFreqWordsDf.style.bar(subset=[\"token\",\"count\"], color='#4286f4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF method for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF gives high scores to terms occurring in only very few documents, and low scores for terms occurring in many documents, so its roughly speaking a measure of how discriminative a term is in a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecTfidf = TfidfVectorizer(\n",
    "        tokenizer=normalize,\n",
    "        stop_words='english',\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),\n",
    "        max_df=1.0, min_df=1, \n",
    "        max_features=None\n",
    "        )\n",
    "mostFreqWords = getFirtsNWords(vecTfidf,df_xml.illness,10)\n",
    "buildPlot(mostFreqWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Grams\n",
    "In this section we will compute word frequencies and the tfidf scores for 2 grams.\n",
    "The structure is the same as 1-gram but in this case we will change the ngram_range to (2,2). That will result in counting 2-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2Gram = CountVectorizer(\n",
    "        tokenizer=normalize,\n",
    "        stop_words='english',\n",
    "        analyzer='word',\n",
    "        ngram_range=(2, 2),\n",
    "        max_df=1.0, min_df=1, \n",
    "        max_features=None\n",
    "        )\n",
    "mostFreq2Grams = getFirtsNWords(vec2Gram,df_xml.illness,10)\n",
    "buildPlot(mostFreq2Grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TfIdf Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecTFIDF2Gram = TfidfVectorizer(\n",
    "        tokenizer=normalize,\n",
    "        stop_words='english',\n",
    "        analyzer='word',\n",
    "        ngram_range=(2, 2),\n",
    "        max_df=1.0, min_df=1, \n",
    "        max_features=None\n",
    "        )\n",
    "mostFreqTFIDF2Grams = getFirtsNWords(vecTFIDF2Gram,df_xml.illness,10)\n",
    "buildPlot(mostFreqTFIDF2Grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Implementation (First method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achive this we need to customize our Vectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we create a customized CountVectorizer. Problem here is that we cannot decide how many tokens can be skiped, or at least I haven't understood that form the example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explemnation what each method does:\n",
    "    - sliding_window(k,tokens)\n",
    "        - k is the number of n-grams and tokens is a list of words.\n",
    "        - Example: sliding_window(3,[\"hello\",\"hi\",\"miki\",\"m\",\"ti\",\"ni\"]) givs us a 3-gram\n",
    "    - pluck(index_array, ngram_zip):\n",
    "        - the index_array gives us the indexes which will be taken into consideration. For Example:\n",
    "            - array = [\"one\",\"2\",\"three\"] and the index_array = [0,2] then the array[1] value will be excluded\n",
    "    - compose(f,g)(x):\n",
    "        - composes functions -> f(g(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem\n",
    "Problem here is with the pluck method. It is static and if we want to skip the first element or the last element and not allways the middle ones it does not work. Better Solution to bee found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2SkipGram = SkipGramVectorizer(\n",
    "    tokenizer=normalize,\n",
    "    stop_words='english',\n",
    "    analyzer='word',\n",
    "    max_df=1.0, min_df=1, \n",
    "    max_features=None\n",
    ")\n",
    "\n",
    "mostFreq2SkipGrams = getFirtsNWords(vec2SkipGram,df_xml.illness,10)\n",
    "buildPlot(mostFreq2SkipGrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create the same implementation for the tfidfVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramTFIDFVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecTFIDF2SkipGram = SkipGramTFIDFVectorizer(\n",
    "    tokenizer=normalize,\n",
    "    stop_words='english',\n",
    "    analyzer='word',\n",
    "    max_df=1.0, min_df=1, \n",
    "    max_features=None\n",
    ")\n",
    "\n",
    "mostFreqTFIDF2SkipGrams = getFirtsNWords(vecTFIDF2SkipGram,df_xml.illness,10)\n",
    "buildPlot(mostFreqTFIDF2SkipGrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better solution for skip gram is the next method.\n",
    "NLTK has a library for skipgrams (from nltk.util import ngrams).\n",
    "The above code has been mostly oriented on the solution presented on https://stackoverflow.com/questions/39725052/is-there-any-way-implementing-skip-gram-with-scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Implementation (Method 2)\n",
    "In this phase we will use a combination of nltk skipgram and assign it as an analyser function. We also need to change the normalize function so that it works for our data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalizeArray function is a extenction of normalize function which we did define on the normalize section.\n",
    "The reason why we need to change the normalize function is bacause the skipper function only accepts 2d lists of tokens and not a 1d list of sentences as by normalize function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the normalize function for Skipgrams.\n",
    "# change was needed bacause the skipper function accepts only 2d lists of tokens and not sentences.\n",
    "def normalizeArray(data):\n",
    "    text = [word_tokenize(line.strip()) for line in data]\n",
    "    newText = []\n",
    "    for tokenArray in text:\n",
    "        tokens = [w.lower() for w in tokenArray]\n",
    "        import string\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        # stemming of words\n",
    "        porter = PorterStemmer()\n",
    "        stemmed = [porter.stem(word) for word in words]\n",
    "        newText.append(stemmed)\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the skipper funtion. We take skipgrams function from nltk.util. The skipgrams function accepts 2 parameters.\n",
    "n: is the number of n-grams. Minimum is 2 bacause using skipgrams for 1-grams does not make sense. \n",
    "k: is the number of allowed tokens to be skiped. It is randomized and can be in the scale of 0 and windows_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "skipper = functools.partial(skipgrams, n=2, k=random.randint(0,window_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firsly we need to apply the normalizeArray function to our data and we save that to a variable called text. We will be able to use this varible multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = normalizeArray(df_xml.illness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 3 lines are the same as what we saw before. Also we create a vectorizer object. In this case we define the analyser to be skipper. Than we get the list of zips by using the getFirstNWords function and give that as an input to the bulidPlot function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerRealData = CountVectorizer(analyzer=skipper,stop_words='english')\n",
    "testVectRealData = getFirtsNWords(vectorizerRealData,text,10)\n",
    "buildPlot(testVectRealData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the same can be done also for the tfidf method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerTFIDFRealData = TfidfVectorizer(analyzer=skipper,stop_words='english')\n",
    "testVectTFIDFRealData = getFirtsNWords(vectorizerTFIDFRealData,text,10)\n",
    "buildPlot(testVectTFIDFRealData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Toy examlpe for the test dataset example, in order to explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "In this week the main focus is to construct the graph dataframe. We will use NetworkX for that reason.\n",
    "Firslty we will do some experimentations in order to understand how NetworkX works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly start with a limited data. We take only 10 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [(('year', 'old'), 202), (('patient', 'year'), 139), (('medic', 'center'), 132), (('patient', 'old'), 125), (('chest', 'pain'), 105), (('short', 'breath'), 87), (('prior', 'admiss'), 83), (('old', 'femal'), 81), (('patient', 'histori'), 80), (('day', 'admiss'), 78)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for this small examlpe we create a function to add nodes on the networX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEdges(data,graph):\n",
    "    for element in data:\n",
    "        graph.add_edge(element[0][0],element[0][1],weight=element[1])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "addEdges(d,G)\n",
    "plt.figure(figsize=(15,10))\n",
    "pos = nx.spring_layout(G, k=1,iterations=10)  # positions for all nodes\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=5000)\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G, pos, width=10)\n",
    "\n",
    "# labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=20, font_family='sans-serif')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to test with real data we will try to make our work a little easier in wich we create a pandas dataframe for our graph. The dataframe has these columns:\n",
    "\n",
    "1. Node1 -> the source\n",
    "2. Node2 -> the target\n",
    "3. Weigth -> the edge weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataframe(data):\n",
    "    node1 = []\n",
    "    node2 = []\n",
    "    weight = []\n",
    "    for entry in data:\n",
    "        node1.append(entry[0][0])\n",
    "        node2.append(entry[0][1])\n",
    "        weight.append(entry[1])\n",
    "    df = pd.DataFrame()\n",
    "    df['Node1']=node1\n",
    "    df['Node2']=node2\n",
    "    df['Weight']= weight\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the dataframe into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = createDataframe(testVectRealData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create the graph and save it at GF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GF = nx.from_pandas_edgelist(df_graph, 'Node1', 'Node2', [\"Weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will start with d3 implementation. For that reason we need a json file for our data. The rest will be in javascript and it will be uploaded on a webserver.\n",
    "\n",
    "We create a node-link format and save it at d and the d content is writen on force.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write json formatted data\n",
    "d = json_graph.node_link_data(GF)  # node-link format to serialize\n",
    "# write json\n",
    "json.dump(d, open('force.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node importance\n",
    "##### As seen on https://necromuralist.github.io/data_science/posts/node-importance/\n",
    "\n",
    "this is some extra data that we will be able to use and give a better visualisation from D3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(GF)\n",
    "closeness_centrality = nx.closeness_centrality(GF)\n",
    "# write json\n",
    "degree_centrality_json = {\"degree_centrality\":degree_centrality}\n",
    "closeness_centrality_json = {\"closeness_centrality\":closeness_centrality}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend force.json\n",
    "We do this so that we have in one file all the needed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('force.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data.update(degree_centrality_json)\n",
    "data.update(closeness_centrality_json)\n",
    "\n",
    "with open('force.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for manipulating json data\n",
    "This function will include the previosly mentioned steps. Firstly creating the host json file in which the nodes the edges and the weigths are contained. Then the second step consist on extending this host json file with properties of network like the two centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNetworkVisualisationData(name,graph):\n",
    "    dataExtension = \"json\"\n",
    "    name = name + \".\" + dataExtension\n",
    "    data = json_graph.node_link_data(graph)\n",
    "    degree_centrality = nx.degree_centrality(GF)\n",
    "    closeness_centrality = nx.closeness_centrality(GF)\n",
    "    degree_centrality_json = {\"degree_centrality\":degree_centrality}\n",
    "    closeness_centrality_json = {\"closeness_centrality\":closeness_centrality}\n",
    "    data.update(degree_centrality_json)\n",
    "    data.update(closeness_centrality_json)\n",
    "    json.dump(data, open(name, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "generateNetworkVisualisationData(\"figaro\",GF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation can be seen on http://eastsidegame.club/bachelor\n",
    "A little hint: The data on server will be updated reguraly util we dicide the visualisation is optimal. For that reason is allways good to clean your caches/cookes before you access the page or simply open the page at a incognito tab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "Here we are still going to work on optimizing our data and making the network vizualisation better. \n",
    "Other features will be normaly added afer we get a feedback from the professor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connected components relation to the thrashhold t\n",
    "A connected component of an undirected graph is a maximal set of nodes such that each pair of nodes is connected by a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerRealData = CountVectorizer(analyzer=skipper,stop_words='english')\n",
    "mostFreqWords2 = getFirstWordsWithCountGreater(vectorizerRealData,text,0)\n",
    "df_graph2 = createDataframe(mostFreqWords2)\n",
    "GF2 = nx.from_pandas_edgelist(df_graph2, 'Node1', 'Node2', [\"Weight\"])\n",
    "graphs = nx.number_connected_components(GF2)\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectedComponentsToThrashold (n):\n",
    "    result = []\n",
    "    connectedComponentsNumberArray = []\n",
    "    tresholdArray = []\n",
    "    for x in range(n+1):\n",
    "        mostFreqWords2 = getFirstWordsWithCountGreater(vectorizerRealData,text,x)\n",
    "        df_graph2 = createDataframe(mostFreqWords2)\n",
    "        GF2 = nx.from_pandas_edgelist(df_graph2, 'Node1', 'Node2', [\"Weight\"])\n",
    "        \n",
    "        # create multiple json graph based on the threshold n\n",
    "        #fileName=\"figaro\"+ str(x)\n",
    "        #generateNetworkVisualisationData(fileName,GF)\n",
    "        \n",
    "        connected_components_number = nx.number_connected_components(GF2)\n",
    "        connectedComponentsNumberArray.append(connected_components_number)\n",
    "        tresholdArray.append(x)\n",
    "    result.append(tresholdArray)\n",
    "    result.append(connectedComponentsNumberArray)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltCoordiantesCCtoTh = connectedComponentsToThrashold(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Connected Components')\n",
    "indexes = np.arange(len(pltCoordiantesCCtoTh[0]),step=1)\n",
    "plt.plot(pltCoordiantesCCtoTh[0], pltCoordiantesCCtoTh[1],LineWidth=1)\n",
    "z = np.polyfit(pltCoordiantesCCtoTh[0], pltCoordiantesCCtoTh[1], 2)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(pltCoordiantesCCtoTh[0],p(pltCoordiantesCCtoTh[0]),\"r\",LineWidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph diameter relation to threshold\n",
    "The diameter of a graph is the maximum eccentricity of any vertex in the graph. That is, it is the greatest distance between any pair of vertices. To find the diameter of a graph, first find the shortest path between each pair of vertices. The greatest length of any of these paths is the diameter of the graph.\n",
    "\n",
    "###### problem if graph is disconected\n",
    "if graph is disconected the diameter is infinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diameterToThrashold (n):\n",
    "    import sys\n",
    "    result = []\n",
    "    diameterArray = []\n",
    "    tresholdArray = []\n",
    "    for x in range(n+1):\n",
    "        mostFreqWords2 = getFirstWordsWithCountGreater(vectorizerRealData,text,x)\n",
    "        df_graph2 = createDataframe(mostFreqWords2)\n",
    "        GF2 = nx.from_pandas_edgelist(df_graph2, 'Node1', 'Node2', [\"Weight\"])\n",
    "        largest_connected_component = max(nx.connected_component_subgraphs(GF2), key=len)\n",
    "        diameter = nx.diameter(largest_connected_component)\n",
    "        diameterArray.append(diameter)\n",
    "        tresholdArray.append(x)\n",
    "    result.append(tresholdArray)\n",
    "    result.append(diameterArray)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltCoordiantesDitoTh = diameterToThrashold(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Diameter')\n",
    "plt.plot(pltCoordiantesDitoTh[0], pltCoordiantesDitoTh[1],LineWidth=1)\n",
    "z = np.polyfit(pltCoordiantesDitoTh[0], pltCoordiantesDitoTh[1], 3)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(pltCoordiantesDitoTh[0],p(pltCoordiantesDitoTh[0]),\"r\",LineWidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density as a relation to threshold\n",
    "density = no of edges/total no of possible edges.\n",
    "\n",
    "d=2m/n(n−1), where m is the number of edges and n is the number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densityToThrashold (n):\n",
    "    import sys\n",
    "    result = []\n",
    "    densityArray = []\n",
    "    tresholdArray = []\n",
    "    for x in range(n+1):\n",
    "        mostFreqWords2 = getFirstWordsWithCountGreater(vectorizerRealData,text,x)\n",
    "        df_graph2 = createDataframe(mostFreqWords2)\n",
    "        GF2 = nx.from_pandas_edgelist(df_graph2, 'Node1', 'Node2', [\"Weight\"])\n",
    "        density = nx.density(GF2)\n",
    "        densityArray.append(density)\n",
    "        tresholdArray.append(x)\n",
    "    result.append(tresholdArray)\n",
    "    result.append(densityArray)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltCoordiantesDentoTh = densityToThrashold(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Density')\n",
    "plt.plot(pltCoordiantesDentoTh[0], pltCoordiantesDentoTh[1])\n",
    "z = np.polyfit(pltCoordiantesDentoTh[0], pltCoordiantesDentoTh[1], 2)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(pltCoordiantesDentoTh[0],p(pltCoordiantesDentoTh[0]),\"r\",LineWidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterCoeffToThrashold (n):\n",
    "    import sys\n",
    "    result = []\n",
    "    clusterArray = []\n",
    "    tresholdArray = []\n",
    "    for x in range(n+1):\n",
    "        mostFreqWords2 = getFirstWordsWithCountGreater(vectorizerRealData,text,x)\n",
    "        df_graph2 = createDataframe(mostFreqWords2)\n",
    "        GF2 = nx.from_pandas_edgelist(df_graph2, 'Node1', 'Node2', [\"Weight\"])\n",
    "        clustercoeff = nx.average_clustering(GF2)\n",
    "        clusterArray.append(clustercoeff)\n",
    "        tresholdArray.append(x)\n",
    "    result.append(tresholdArray)\n",
    "    result.append(clusterArray)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltCoordiantesClustoTh = clusterCoeffToThrashold(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Cluster Coefficient')\n",
    "plt.plot(pltCoordiantesClustoTh[0], pltCoordiantesClustoTh[1])\n",
    "z = np.polyfit(pltCoordiantesClustoTh[0], pltCoordiantesClustoTh[1], 2)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(pltCoordiantesClustoTh[0],p(pltCoordiantesClustoTh[0]),\"r\",LineWidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(GF2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "parts = community.girvan_newman(GF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the normalize function for Skipgrams in case we want to do part of speech tagging\n",
    "def normalizeArrayPartOfSpeech(data):\n",
    "    text = [word_tokenize(line.strip()) for line in data]\n",
    "    newText = []\n",
    "    for tokenArray in text:\n",
    "        tokens = [w.lower() for w in tokenArray]\n",
    "        import string\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        \n",
    "        #trying lemmatization\n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "        #lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "        #lemmatized=nltk.pos_tag(lemmatized)\n",
    "        #newText.append(lemmatized)\n",
    "        \n",
    "        # stemming of words\n",
    "        porter = PorterStemmer()\n",
    "        stemmed = [porter.stem(word) for word in words]\n",
    "        stemmed=nltk.pos_tag(stemmed)\n",
    "        newText.append(stemmed)\n",
    "        \n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = normalizeArrayPartOfSpeech(df_xml.illness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting the normalized text with the pos tag to a json file\n",
    "this two new files will be used to visualize POS tags in the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePOSTagJson(text):\n",
    "    # convert nd array to 1d\n",
    "    nDimens = np.array(text)\n",
    "    oneDimens = np.concatenate(nDimens)\n",
    "    # interate throught every entry\n",
    "    data = {}\n",
    "    for entry in oneDimens:\n",
    "        data[entry[0]]=entry[1]\n",
    "    json.dump(data, open(\"POSTag.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatePOSTagJson(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTAGColorJson():\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    data = {}\n",
    "    tags = tagdict.keys()\n",
    "    for tag in tags:\n",
    "        color = \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "        data[tag] = \"#\"+color\n",
    "    json.dump(data, open(\"TAGColor.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateTAGColorJson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## community detection \n",
    "using the louvain method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "partition = community.best_partition(GF)\n",
    "#drawing\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(GF)\n",
    "count = 0.\n",
    "for com in set(partition.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(GF, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(GF, pos, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partitions found: \", len(set(partition.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read graph from json file\n",
    "def readGraphFromJson(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get partitions from saved graphs\n",
    "def getPartitions(filename):\n",
    "    graph = readGraphFromJson(filename)\n",
    "    partition = community.best_partition(graph)\n",
    "    return partition, len(set(partition.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,7):\n",
    "    partition = getPartitions(\"force_\"+str(x)+\"00.json\")\n",
    "    result = {}\n",
    "    result[\"partition\"] = partition[0]\n",
    "    colors = {}\n",
    "    partitionCount = partition[1]\n",
    "    for c in range(partitionCount+1):\n",
    "        color = \"%06x\" % random.randint(0, 0xFFFFFF)\n",
    "        colors[c] = \"#\"+color\n",
    "    result[\"colors\"]= colors\n",
    "    json.dump(result, open(\"force_partitions_\"+str(x)+\"00.json\", 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
